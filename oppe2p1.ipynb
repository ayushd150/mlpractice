{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "83601724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b72553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Preprocessing_V1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "25f750af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 11)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6aa569",
   "metadata": {},
   "source": [
    "What is the total number of missing or unknown values in the column Gender?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bb8409a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Gender'].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "27b76b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender\n",
       "Female     2366\n",
       "Male       1627\n",
       "Unknown       7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Gender'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "87534b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(7)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_count = df['Gender'].isin(['Unknown', 'unknown', '?']).sum()\n",
    "unknown_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ccb56",
   "metadata": {},
   "source": [
    "What is the total number of missing or unknown values in the column GlucoseLevel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "48cb04b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 54.6 108.8  64.1 ... 218.9  66.3 168.5]\n"
     ]
    }
   ],
   "source": [
    "print(df['GlucoseLevel'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "27e70118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['GlucoseLevel'].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2a18d6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing or unknown GlucoseLevel values: 0\n"
     ]
    }
   ],
   "source": [
    "missing_or_unknown = df['GlucoseLevel'].replace(\n",
    "    ['Unknown', 'unknown', '?', '', 'NA', 'N/A', 'na', 'null', 'NULL'], \n",
    "    np.nan\n",
    ").isna().sum()\n",
    "\n",
    "print(\"Total missing or unknown GlucoseLevel values:\", missing_or_unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c84a6",
   "metadata": {},
   "source": [
    "What is the total number of missing or unknown values in the column LivesIn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ea9fd1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['City' 'Village' 'Unknown']\n"
     ]
    }
   ],
   "source": [
    "print(df['LivesIn'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f13ccce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing or unknown  5\n"
     ]
    }
   ],
   "source": [
    "missing_or_unknown = df['LivesIn'].replace(\n",
    "    ['Unknown'], \n",
    "    np.nan\n",
    ").isna().sum()\n",
    "\n",
    "print(\"Total missing or unknown \", missing_or_unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c82f5",
   "metadata": {},
   "source": [
    "What is the total number of missing or unknown values in the column BMI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18fa4ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35.1 26.7 23.4 27.4 41.6 29.3 37.1 16.1 40.5 15.8 29.9 29.5 40.7 36.6\n",
      " 31.5  nan 12.1 25.5 22.3 27.1 44.7 26.1 18.8 18.7 24.8 17.  37.6 20.6\n",
      " 29.  56.2 30.7 25.3 23.  27.2 19.2 31.6 24.6 27.  24.5 18.2 52.  32.3\n",
      " 42.7 30.  24.3 24.2 25.7 36.7 46.4 48.3 20.9 24.7 23.6 26.5 39.4 18.4\n",
      " 25.6 25.9 54.6 31.9 14.6 38.7 23.7 27.3 29.2 39.7 30.1 28.1 35.7 14.3\n",
      " 30.4 22.2 35.  44.5 36.3 25.2 26.6 31.4 36.8 25.8 38.4 43.2 20.4 30.6\n",
      " 33.8 34.  26.2 29.6 30.2 22.9 38.9 16.3 23.3 25.1 34.1 45.7 37.3 26.4\n",
      " 40.9 31.1 17.7 27.5 19.9 32.  35.9 32.1 24.9 23.8 18.  20.7 27.7 22.6\n",
      " 13.1 19.4 28.5 28.8 21.7 19.6 27.8 41.  41.8 35.2 44.4 42.6 15.7 52.8\n",
      " 23.1 38.5 22.7 18.3 42.3 43.4 51.5 24.  28.7 23.9 37.9 32.6 35.6 34.7\n",
      " 28.3 33.2 32.5 44.1 34.2 22.  33.3 16.8 35.4 20.1 26.3 37.5 33.1 21.2\n",
      " 33.  33.6 30.3 26.  34.8 31.8 42.4 25.4 23.2 19.3 27.9 36.1 43.  16.9\n",
      " 20.5 33.9 28.2 24.1 41.1 32.2 26.8 30.8 22.5 29.7 40.  34.5 28.  37.7\n",
      " 19.8 28.4 34.3 20.8 16.2 17.5 36.9 19.5 31.7 34.4 29.1 39.3 35.5 21.\n",
      " 31.  25.  20.3 17.9 36.5 47.5 19.  23.5 38.8 39.5 22.1 30.5 29.4 32.4\n",
      " 16.7 22.8 16.4 24.4 54.8 19.1 39.2 18.5 28.6 48.2 41.2 20.  34.6 36.4\n",
      " 29.8 59.7 14.4 28.9 27.6 19.7 32.8 44.8 21.1 16.6 14.9 18.9 17.1 20.2\n",
      " 33.7 38.2 55.6 21.5 32.9 40.3 18.1 38.6 15.1 41.5 21.9 39.6 42.  13.7\n",
      " 38.  41.3 35.3 48.6 41.7 21.3 47.6 30.9 61.1 31.3 38.3 37.2 35.8 49.1\n",
      " 33.5 18.6 44.3 26.9 17.2 31.2 44.  49.8 39.1 39.8 39.9 17.3 22.4 21.4\n",
      " 40.2 33.4 56.5 37.4 17.4 16.5 21.6 41.4 37.  36.  43.7 13.3 17.8 14.8\n",
      " 39.  40.8 48.4 43.8 63.6 36.2 42.5 40.1 43.9 15.4 13.2 43.5 58.7 14.\n",
      " 46.8 43.6 37.8 46.3 45.4 15.2 17.6 32.7 46.1 42.1 58.5 45.9 41.9 50.7\n",
      " 54.  21.8 47.  66.1 45.3 34.9 42.2 55.7 55.1 45.1 52.7 16.  54.2 40.6\n",
      " 49.9 13.8 53.4 46.9 55.8 45.6 43.3 15.3 77.9 47.3 38.1 57.9 53.6  9.6\n",
      " 15.5 49.2 45.  49.5 12.8 62.6 43.1 12.2 56.8 45.2 51.9 52.1 62.5 47.8\n",
      " 51.3 45.5 10.9 47.4 49.7 49.4 48.  50.8 52.4 45.8 97.  13.6 48.9 57.3\n",
      " 14.5 40.4 47.9 50.5 12.6 14.1 57.4 42.8 49.3 46.2 48.7 58.4 53.3 55.\n",
      " 46.5 53.1 51.8 50.3 53.9 58.1 13.  52.2 15.9 13.9 51.  51.4 57.6 46.7\n",
      " 53.  15.  14.7]\n"
     ]
    }
   ],
   "source": [
    "print(df['BMI'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3e9b41eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "unknown = df['BMI'].replace(['nan'], np.nan).isna().sum()\n",
    "print(unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e60f6870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(149)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['BMI'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62e1098",
   "metadata": {},
   "source": [
    "What is the total number of missing or unknown values in the column SmokingStatus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "28b5a1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['never smoked' 'smokes' 'Unknown' 'formerly smoked']\n"
     ]
    }
   ],
   "source": [
    "print(df['SmokingStatus'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8126e393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1204\n"
     ]
    }
   ],
   "source": [
    "value = df['SmokingStatus'].replace(['Unknown'], np.nan).isna().sum()\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549c3c0",
   "metadata": {},
   "source": [
    "What is the mean value of the BMI in the dataset? Ignore the missing values if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5d6d6283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(28.857958971695663)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['BMI'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93baae05",
   "metadata": {},
   "source": [
    "How many people live in city, smoked at least once in life and had a heartattack? Ignore records/rows with any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b55fe4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['never smoked' 'smokes' 'Unknown' 'formerly smoked']\n",
      "['No' 'Yes']\n",
      "['City' 'Village' 'Unknown']\n"
     ]
    }
   ],
   "source": [
    "print(df['SmokingStatus'].unique())\n",
    "print(df['HeartAttack'].unique())\n",
    "print(df['LivesIn'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e9c69b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "cond = df[(df['LivesIn'] == 'City')&(df['HeartAttack'] == 'Yes')&(df['SmokingStatus'].isin(['smokes', 'formerly smoked']))]\n",
    "count = cond.shape[0]\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9301fb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1295, 732, 799, 18)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df.dropna()\n",
    "\n",
    "# A: female, no tension, no heart disease, never married\n",
    "A = df_clean[\n",
    "    (df_clean[\"Gender\"] == \"Female\") &\n",
    "    (df_clean[\"HasTension\"] == \"No\") &\n",
    "    (df_clean[\"HeartAttack\"] == \"No\") &\n",
    "    (df_clean[\"NeverMarried\"] == \"Yes\")\n",
    "].shape[0]\n",
    "\n",
    "# B: female, no tension, no heart disease, married or previously married\n",
    "B = df_clean[\n",
    "    (df_clean[\"Gender\"] == \"Female\") &\n",
    "    (df_clean[\"HasTension\"] == \"No\") &\n",
    "    (df_clean[\"HeartAttack\"] == \"No\") &\n",
    "    (df_clean[\"NeverMarried\"] == \"No\")\n",
    "].shape[0]\n",
    "\n",
    "# C: male, no tension, no heart disease, never married\n",
    "C = df_clean[\n",
    "    (df_clean[\"Gender\"] == \"Male\") &\n",
    "    (df_clean[\"HasTension\"] == \"No\") &\n",
    "    (df_clean[\"HeartAttack\"] == \"No\") &\n",
    "    (df_clean[\"NeverMarried\"] == \"Yes\")\n",
    "].shape[0]\n",
    "\n",
    "# D: male, tension = yes, heart attack = yes, never married\n",
    "D = df_clean[\n",
    "    (df_clean[\"Gender\"] == \"Male\") &\n",
    "    (df_clean[\"HasTension\"] == \"Yes\") &\n",
    "    (df_clean[\"HeartAttack\"] == \"Yes\") &\n",
    "    (df_clean[\"NeverMarried\"] == \"Yes\")\n",
    "].shape[0]\n",
    "\n",
    "A, B, C, D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c92dec",
   "metadata": {},
   "source": [
    "'HeartAttack' is the target column. What is the distribution count of \"No\" and \"Yes\" classes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ab2d6a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeartAttack\n",
       "No     3806\n",
       "Yes     194\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.HeartAttack.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc39b5c",
   "metadata": {},
   "source": [
    "Divide the data into training and test sets\n",
    "Keep 30% of the data as test set.\n",
    "\n",
    "Use random_state as 0\n",
    "\n",
    "HeartAttack is the target, rest of the columns are the features.\n",
    "\n",
    "For the label/target vector, replace \"Yes\" with 1 and \"No\" with 0.\n",
    "\n",
    "Divide the dataset into training and test sets keeping target(y) in stratified manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7420b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "df['HeartAttack'] = df.HeartAttack.map({'Yes': 1, 'No': 0})\n",
    "X = df.drop('HeartAttack', axis=1)\n",
    "y = df['HeartAttack']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "591a670f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2800, 10)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b7926",
   "metadata": {},
   "source": [
    "Prepare a data preprocessing pipeline to process features in following order:\n",
    "Gender: Impute with most frequent then ordinally encode.\n",
    "Age: Impute with mean then standard scale.\n",
    "HasTension: Ordinally encode.\n",
    "AnyHeartDisease:Ordinally encode.\n",
    "NeverMarried:Ordinally encode.\n",
    "Occupation: One hot encode.\n",
    "LivesIn: Impute with most frequent then ordinally encode.\n",
    "GlucoseLevel: Impute with mean, then min-max scaling.\n",
    "BMI: Impute with mean, then standard scale.\n",
    "SmokingStatus: Impute with most frequent, then one hot encode.\n",
    "Hint: After transformation, your feature matrix must have columns in following order:\n",
    "\n",
    "Gender\n",
    "Age\n",
    "HasTension\n",
    "AnyHeartDisease\n",
    "NeverMarried\n",
    "Occupation_Govt_job\n",
    "Occupation_Never_worked\n",
    "Occupation_Private\n",
    "Occupation_Self-employed\n",
    "Occupation_children\n",
    "LivesIn\n",
    "GlucoseLevel\n",
    "BMI\n",
    "SmokingStatus_formerly smoked\n",
    "SmokingStatus_never smoked\n",
    "SmokingStatus_smokes\n",
    "NOTE:\n",
    "\n",
    "Make sure to preprocess the features in the above order exactly. Answer(s) of later question(s) depend(s) upon correct order of featuring processing.\n",
    "You may have to use multiple instances of a trasnformer for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a82dbc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_modified = X_train.copy()\n",
    "X_test_modified = X_test.copy()\n",
    "X_train_modified['SmokingStatus'] = X_train_modified['SmokingStatus'].replace('Unknown', np.nan)\n",
    "X_test_modified['SmokingStatus'] = X_test_modified['SmokingStatus'].replace('Unknown', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "25694815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Individual Preprocessing Steps ---\n",
    "\n",
    "# 1. Gender: Impute (most frequent) then Ordinal Encode\n",
    "gender_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "# 2. Age: Impute (mean) then Standard Scale\n",
    "age_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 3, 4, 5: HasTension, AnyHeartDisease, NeverMarried: Ordinal Encode\n",
    "tension_pipe = OrdinalEncoder()\n",
    "heart_pipe = OrdinalEncoder()\n",
    "married_pipe = OrdinalEncoder()\n",
    "\n",
    "# 6. Occupation: One hot encode (handle_unknown='ignore' to avoid errors on new categories)\n",
    "occupation_pipe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# 7. LivesIn: Impute (most frequent) then Ordinal Encode\n",
    "livesin_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "# 8. GlucoseLevel: Impute (mean) then Min-Max Scale\n",
    "glucose_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# 9. BMI: Impute (mean) then Standard Scale\n",
    "bmi_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 10. SmokingStatus: Impute (most frequent) then One hot encode\n",
    "smoking_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# --- Define the ColumnTransformer ---\n",
    "# The order of the features below dictates the order of columns in the final matrix.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('gender_pipe', gender_pipe, ['Gender']),\n",
    "        ('age_pipe', age_pipe, ['Age']),\n",
    "        ('tension_pipe', tension_pipe, ['HasTension']),\n",
    "        ('heart_pipe', heart_pipe, ['AnyHeartDisease']),\n",
    "        ('married_pipe', married_pipe, ['NeverMarried']),\n",
    "        ('occupation_pipe', occupation_pipe, ['Occupation']),\n",
    "        ('livesin_pipe', livesin_pipe, ['LivesIn']),\n",
    "        ('glucose_pipe', glucose_pipe, ['GlucoseLevel']),\n",
    "        ('bmi_pipe', bmi_pipe, ['BMI']),\n",
    "        ('smoking_pipe', smoking_pipe, ['SmokingStatus'])\n",
    "    ],\n",
    "    remainder='drop' # Ensures no unintended columns are passed through\n",
    ")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train_modified)\n",
    "\n",
    "# The shape of the processed training feature matrix is (2800, 16)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e57859f",
   "metadata": {},
   "source": [
    "Calculate the shape of the feature matrix of training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9f0e9290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2800, 16)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771743c7",
   "metadata": {},
   "source": [
    "What is the mean of the transformed test data (features only)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "236de0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of transformed test data: 0.26833482647759943\n"
     ]
    }
   ],
   "source": [
    "# Transform the test set using the *already fitted* preprocessor\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Compute the mean of ALL values in the transformed test matrix\n",
    "mean_test = X_test_processed.mean()\n",
    "\n",
    "print(\"Mean of transformed test data:\", mean_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c5bdd",
   "metadata": {},
   "source": [
    "If you eliminate 1 feature with recursive feature elimination, which feature will be eliminated?\n",
    "Type the index of the eliminated feature (index starts from 0).\n",
    "Use LogisticRegression model with random state as 1729 and rest of the parameters with default values, as an estimator.\n",
    "Use processed training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3062eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e9bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminated feature index: [np.True_, np.True_, np.True_, np.True_, np.True_, np.True_, np.True_, np.True_, np.True_, np.True_, np.True_, np.True_, np.True_, np.True_, np.True_, np.False_]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Logistic Regression model\n",
    "model = LogisticRegression(random_state=1729)\n",
    "\n",
    "# RFE: eliminate 1 feature\n",
    "rfe = RFE(estimator=model, n_features_to_select=X_train.shape[1] - 1)\n",
    "\n",
    "# Fit RFE\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get eliminated feature index (the one marked False)\n",
    "eliminated_feature_index = list(rfe.support_).index(False)\n",
    "\n",
    "print(\"Eliminated feature index:\", eliminated_feature_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cc2ee57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ModelBuilding_V1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92327c",
   "metadata": {},
   "source": [
    "Load the dataset.\n",
    "\n",
    "The last column is the target column.\n",
    "Last 30% rows of the dataset constitute test set and remaining rows form the training set.\n",
    "Do not shuffle the dataset while splitting\n",
    "You must have to use only training set to train all the estimator in questions below.\n",
    "First row of the file has column names/ids, and it has no index column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882974cc",
   "metadata": {},
   "source": [
    "Instantiate a perceptron classifier that with following parameters:\n",
    "\n",
    "random_state = 1729\n",
    "learning rate = 1\n",
    "Train for appropriate number of iterations\n",
    "Do not shuffle the dataset for each iteration.\n",
    "Include the intercept (bias) term.\n",
    "Use 10% of the data for validation fraction.\n",
    "Do not apply regularization.\n",
    "Set warm start to true.\n",
    "Hint: one iteration of training indicates going over each sample exactly once.\n",
    "\n",
    "Train the classifier on the training data.\n",
    "\n",
    "Q.Train the perceptron classifier for 5 iterations. What is value of bias (intercept) after 5th iteration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "447ca378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "82617489",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Perceptron(random_state=1729, shuffle=False, eta0=1.0, validation_fraction=0.1, penalty=None, warm_start=True , fit_intercept=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3a2b93b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias (intercept) after 5th iteration: -12.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('ModelBuilding_V1.csv')\n",
    "\n",
    "# Separate features and target (last column is target)\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Split without shuffling - last 30% for test, first 70% for train\n",
    "split_index = int(len(df) * 0.7)\n",
    "X_train = X.iloc[:split_index]\n",
    "y_train = y.iloc[:split_index]\n",
    "X_test = X.iloc[split_index:]\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "# Handle missing values in features\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Standardize the features (recommended for Perceptron)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Handle missing values in target (if any)\n",
    "if y_train.isna().any():\n",
    "    valid_indices = ~y_train.isna()\n",
    "    X_train_scaled = X_train_scaled[valid_indices]\n",
    "    y_train = y_train[valid_indices]\n",
    "\n",
    "# Instantiate Perceptron with specified parameters\n",
    "perceptron = Perceptron(\n",
    "    random_state=1729,\n",
    "    eta0=1,  # learning rate = 1\n",
    "    max_iter=5,  # Train for 5 iterations\n",
    "    shuffle=False,  # Do not shuffle for each iteration\n",
    "    fit_intercept=True,  # Include intercept (bias) term\n",
    "    validation_fraction=0.1,  # 10% validation fraction\n",
    "    alpha=0.0,  # No regularization\n",
    "    warm_start=True,  # Set warm start to true\n",
    "    early_stopping=False,  # Disable early stopping to ensure 5 iterations\n",
    "    tol=None  # No tolerance for stopping\n",
    ")\n",
    "\n",
    "# Train the perceptron for 5 iterations\n",
    "perceptron.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the bias (intercept) after 5th iteration\n",
    "bias_after_5_iterations = perceptron.intercept_[0]\n",
    "\n",
    "print(f\"Bias (intercept) after 5th iteration: {bias_after_5_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9cf0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
